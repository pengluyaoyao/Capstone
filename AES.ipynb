{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Automated Essay Scoring (AES)\n",
    "\n",
    "Essays are crucial testing tools for assessing academic achievement, integration of ideas and ability, but are expensive and time consuming to grade manually. Automated essay scoring (AES) saves the efforts of human graders and hence significantly reduces costs and time. In some high stakes examinations, AES is used so there is no need to have a second human grader to verify or compare; In low stakes evaluations, AES is the only grading scheme. \n",
    "\n",
    "Previous studies have included baseline features:\n",
    "1. Bag of Words (BOW) counts (10000 words with maximum frequency)\n",
    "2. Number of characters \n",
    "3. Number of words \n",
    "4. Number of sentences \n",
    "5. Average word length\n",
    "6. Number of lemmas\n",
    "7. Number of spellng errors \n",
    "8. Number of nouns\n",
    "9. Number of adjectives \n",
    "10. Number of verbs \n",
    "11. Number of adverbs \n",
    "\n",
    "In addition to those features, this project tries to extract \n",
    "1. sentiment features\n",
    "2. content features\n",
    "3. grammar features \n",
    "\n",
    "## Goal\n",
    "\n",
    "1. What kind of features could imporve AES models so it grades as close as to the human graders. \n",
    "\n",
    "2. Different AES models will also be compared. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data\n",
    "\n",
    "## Dataset from kaggle.com, by the William and Flora Hewlett Foundation.\n",
    "\n",
    "|Type of Essay|grade level|number of training |number of validation|\n",
    "|----------|-------------------|--------------|------|\n",
    "|narrative/persuasive/expository|8|1785|592 \n",
    "|narrative/persuasive/expository|10|1800|600| \n",
    "|source dependent|10|1726|575|\n",
    "|source dependent|10|1772|589|\n",
    "|source dependent|8|1805|601|\n",
    "|source dependent|10|1800|600|\n",
    "|narrative/persuasive/expository|7|1730|576|\n",
    "|narrative/persuasive/expository|10|918|305|\n",
    "\n",
    "## Example: Essay set 1 prompt\n",
    "\n",
    "\n",
    "More and more people use computers, but not everyone agrees that this benefits society. Those who support advances in technology believe that computers have a positive effect on people. They teach hand-eye coordination, give people the ability to learn about faraway places and people, and even allow people to talk online with other people. Others have different ideas. Some experts are concerned that people are spending too much time on their computers and less time exercising, enjoying nature, and interacting with family and friends. \n",
    "\n",
    "Write a letter to your local newspaper in which you state your opinion on the effects computers have on people. Persuade the readers to agree with you.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Descriptive Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/pengluyao/anaconda3/lib/python3.6/importlib/_bootstrap.py:219: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility. Expected 96, got 88\n",
      "  return f(*args, **kwds)\n",
      "/Users/pengluyao/anaconda3/lib/python3.6/importlib/_bootstrap.py:219: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility. Expected 96, got 88\n",
      "  return f(*args, **kwds)\n",
      "/Users/pengluyao/anaconda3/lib/python3.6/site-packages/sklearn/ensemble/weight_boosting.py:29: DeprecationWarning: numpy.core.umath_tests is an internal NumPy module and should not be imported. It will be removed in a future NumPy release.\n",
      "  from numpy.core.umath_tests import inner1d\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import xlrd\n",
    "import matplotlib.pyplot as plt\n",
    "import re, collections\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from itertools import chain\n",
    "import nltk\n",
    "from nltk.tokenize import sent_tokenize\n",
    "from nltk.tokenize import word_tokenize\n",
    "import string\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import wordnet\n",
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       essay_id  essay_set                                              essay  \\\n",
      "0             1          1  Dear local newspaper, I think effects computer...   \n",
      "1             2          1  Dear @CAPS1 @CAPS2, I believe that using compu...   \n",
      "2             3          1  Dear, @CAPS1 @CAPS2 @CAPS3 More and more peopl...   \n",
      "3             4          1  Dear Local Newspaper, @CAPS1 I have found that...   \n",
      "4             5          1  Dear @LOCATION1, I know having computers has a...   \n",
      "5             6          1  Dear @LOCATION1, I think that computers have a...   \n",
      "6             7          1  Did you know that more and more people these d...   \n",
      "7             8          1  @PERCENT1 of people agree that computers make ...   \n",
      "8             9          1  Dear reader, @ORGANIZATION1 has had a dramatic...   \n",
      "9            10          1  In the @LOCATION1 we have the technology of a ...   \n",
      "10           11          1  Dear @LOCATION1, @CAPS1 people acknowledge the...   \n",
      "11           12          1  Dear @CAPS1 @CAPS2 I feel that computers do ta...   \n",
      "12           13          1  Dear local newspaper I raed ur argument on the...   \n",
      "13           14          1  My three detaileds for this news paper article...   \n",
      "14           15          1  Dear, In this world today we should have every...   \n",
      "15           16          1  Dear @ORGANIZATION1, The computer blinked to l...   \n",
      "16           17          1  Dear Local Newspaper, I belive that computers ...   \n",
      "17           18          1  Dear Local Newspaper, I must admit that the ex...   \n",
      "18           19          1  I aegre waf the evansmant ov tnachnolage. The ...   \n",
      "19           20          1  Well computers can be a good or a bad thing. I...   \n",
      "20           21          1  Dear @CAPS1 of the @CAPS2 @CAPS3 daily, I am w...   \n",
      "21           22          1  Dear local Newspaper @CAPS1 a take all your co...   \n",
      "22           23          1  Dear local newspaper, @CAPS1 you ever see a ch...   \n",
      "23           24          1  Dear local newspaper, I've heard that not many...   \n",
      "24           25          1  Dear @CAPS1, @CAPS2 off, I beileve that comput...   \n",
      "25           26          1  Do you think that computers are useless? Or do...   \n",
      "26           27          1  Computers a good because you can get infermati...   \n",
      "27           28          1  Dear Newspaper, Computers are high tec and hav...   \n",
      "28           29          1  Dear local newspaper, @CAPS1 people throughout...   \n",
      "29           30          1  Dear Newspaper People, I think that computers ...   \n",
      "...         ...        ...                                                ...   \n",
      "12948     21592          8   We all understand the benefits of laughter. L...   \n",
      "12949     21594          8        It was midsummer, and i could feel the c...   \n",
      "12950     21595          8   Have you ever experienced a time with your fr...   \n",
      "12951     21596          8   I woke up just like any other day happy yet l...   \n",
      "12952     21598          8   Laughter is an important part of my life, eit...   \n",
      "12953     21599          8   I sat at the table, speechless, as they told ...   \n",
      "12954     21601          8   As I remember back, it was @DATE1. It was a h...   \n",
      "12955     21603          8   Those eyes, it was like I was looking out int...   \n",
      "12956     21604          8  Some say that laugh is the common language bet...   \n",
      "12957     21605          8   Laughter is an integral element to many situa...   \n",
      "12958     21606          8  One time I was at my friend @PERSON1's house, ...   \n",
      "12959     21607          8   LAUGHTER @CAPS1 knows that laughter is a heal...   \n",
      "12960     21608          8  One thing that people in the world love to do ...   \n",
      "12961     21609          8   Laughter, to me, is an important aspect of my...   \n",
      "12962     21610          8   People always say that the worst parts of lif...   \n",
      "12963     21611          8   Why is it that people can look back at someth...   \n",
      "12964     21613          8   Before my best friend moved away, we would st...   \n",
      "12965     21615          8                                @ORGANIZATION1  ...   \n",
      "12966     21617          8   Morose and somnolent, I woke up. I woke up to...   \n",
      "12967     21618          8   A while back my mom had decided to send me to...   \n",
      "12968     21619          8                              I dont like computers   \n",
      "12969     21620          8   Everyone knows how important a laugh can be. ...   \n",
      "12970     21621          8   Laughter is an important part of my family. W...   \n",
      "12971     21623          8   laughter is an important part of any kind of ...   \n",
      "12972     21624          8  Sometime ago on a hot @DATE1 day my @NUM1 ,@PE...   \n",
      "12973     21626          8   In most stories mothers and daughters are eit...   \n",
      "12974     21628          8   I never understood the meaning laughter is th...   \n",
      "12975     21629          8  When you laugh, is @CAPS5 out of habit, or is ...   \n",
      "12976     21630          8                                 Trippin' on fen...   \n",
      "12977     21633          8   Many people believe that laughter can improve...   \n",
      "\n",
      "       rater1_domain1  rater2_domain1  rater3_domain1  domain1_score  \\\n",
      "0                 4.0             4.0             NaN            8.0   \n",
      "1                 5.0             4.0             NaN            9.0   \n",
      "2                 4.0             3.0             NaN            7.0   \n",
      "3                 5.0             5.0             NaN           10.0   \n",
      "4                 4.0             4.0             NaN            8.0   \n",
      "5                 4.0             4.0             NaN            8.0   \n",
      "6                 5.0             5.0             NaN           10.0   \n",
      "7                 5.0             5.0             NaN           10.0   \n",
      "8                 4.0             5.0             NaN            9.0   \n",
      "9                 5.0             4.0             NaN            9.0   \n",
      "10                4.0             4.0             NaN            8.0   \n",
      "11                4.0             4.0             NaN            8.0   \n",
      "12                4.0             3.0             NaN            7.0   \n",
      "13                3.0             3.0             NaN            6.0   \n",
      "14                3.0             3.0             NaN            6.0   \n",
      "15                6.0             6.0             NaN           12.0   \n",
      "16                4.0             4.0             NaN            8.0   \n",
      "17                4.0             4.0             NaN            8.0   \n",
      "18                2.0             2.0             NaN            4.0   \n",
      "19                3.0             3.0             NaN            6.0   \n",
      "20                4.0             4.0             NaN            8.0   \n",
      "21                2.0             1.0             NaN            3.0   \n",
      "22                5.0             5.0             NaN           10.0   \n",
      "23                6.0             5.0             NaN           11.0   \n",
      "24                4.0             4.0             NaN            8.0   \n",
      "25                5.0             4.0             NaN            9.0   \n",
      "26                2.0             2.0             NaN            4.0   \n",
      "27                5.0             4.0             NaN            9.0   \n",
      "28                5.0             4.0             NaN            9.0   \n",
      "29                4.0             4.0             NaN            8.0   \n",
      "...               ...             ...             ...            ...   \n",
      "12948            20.0            20.0             NaN           40.0   \n",
      "12949            17.0            15.0             NaN           32.0   \n",
      "12950            18.0            18.0             NaN           36.0   \n",
      "12951            15.0            16.0             NaN           31.0   \n",
      "12952            15.0            15.0             NaN           30.0   \n",
      "12953            25.0            22.0             NaN           47.0   \n",
      "12954            20.0            20.0             NaN           40.0   \n",
      "12955            17.0            18.0             NaN           35.0   \n",
      "12956            16.0            17.0             NaN           33.0   \n",
      "12957            18.0            18.0             NaN           36.0   \n",
      "12958            20.0            16.0             NaN           36.0   \n",
      "12959            25.0            23.0             NaN           48.0   \n",
      "12960            20.0            20.0             NaN           40.0   \n",
      "12961            20.0            20.0             NaN           40.0   \n",
      "12962            20.0            20.0             NaN           40.0   \n",
      "12963            22.0            20.0             NaN           42.0   \n",
      "12964            20.0            19.0            40.0           40.0   \n",
      "12965            15.0            17.0             NaN           32.0   \n",
      "12966            18.0            18.0             NaN           36.0   \n",
      "12967            20.0            19.0            40.0           40.0   \n",
      "12968             5.0             5.0             NaN           10.0   \n",
      "12969            18.0            15.0             NaN           33.0   \n",
      "12970            24.0            20.0             NaN           44.0   \n",
      "12971            18.0            17.0             NaN           35.0   \n",
      "12972            17.0            15.0            30.0           30.0   \n",
      "12973            17.0            18.0             NaN           35.0   \n",
      "12974            15.0            17.0             NaN           32.0   \n",
      "12975            20.0            26.0            40.0           40.0   \n",
      "12976            20.0            20.0             NaN           40.0   \n",
      "12977            20.0            20.0             NaN           40.0   \n",
      "\n",
      "       rater1_domain2  rater2_domain2  domain2_score      ...        \\\n",
      "0                 NaN             NaN            NaN      ...         \n",
      "1                 NaN             NaN            NaN      ...         \n",
      "2                 NaN             NaN            NaN      ...         \n",
      "3                 NaN             NaN            NaN      ...         \n",
      "4                 NaN             NaN            NaN      ...         \n",
      "5                 NaN             NaN            NaN      ...         \n",
      "6                 NaN             NaN            NaN      ...         \n",
      "7                 NaN             NaN            NaN      ...         \n",
      "8                 NaN             NaN            NaN      ...         \n",
      "9                 NaN             NaN            NaN      ...         \n",
      "10                NaN             NaN            NaN      ...         \n",
      "11                NaN             NaN            NaN      ...         \n",
      "12                NaN             NaN            NaN      ...         \n",
      "13                NaN             NaN            NaN      ...         \n",
      "14                NaN             NaN            NaN      ...         \n",
      "15                NaN             NaN            NaN      ...         \n",
      "16                NaN             NaN            NaN      ...         \n",
      "17                NaN             NaN            NaN      ...         \n",
      "18                NaN             NaN            NaN      ...         \n",
      "19                NaN             NaN            NaN      ...         \n",
      "20                NaN             NaN            NaN      ...         \n",
      "21                NaN             NaN            NaN      ...         \n",
      "22                NaN             NaN            NaN      ...         \n",
      "23                NaN             NaN            NaN      ...         \n",
      "24                NaN             NaN            NaN      ...         \n",
      "25                NaN             NaN            NaN      ...         \n",
      "26                NaN             NaN            NaN      ...         \n",
      "27                NaN             NaN            NaN      ...         \n",
      "28                NaN             NaN            NaN      ...         \n",
      "29                NaN             NaN            NaN      ...         \n",
      "...               ...             ...            ...      ...         \n",
      "12948             NaN             NaN            NaN      ...         \n",
      "12949             NaN             NaN            NaN      ...         \n",
      "12950             NaN             NaN            NaN      ...         \n",
      "12951             NaN             NaN            NaN      ...         \n",
      "12952             NaN             NaN            NaN      ...         \n",
      "12953             NaN             NaN            NaN      ...         \n",
      "12954             NaN             NaN            NaN      ...         \n",
      "12955             NaN             NaN            NaN      ...         \n",
      "12956             NaN             NaN            NaN      ...         \n",
      "12957             NaN             NaN            NaN      ...         \n",
      "12958             NaN             NaN            NaN      ...         \n",
      "12959             NaN             NaN            NaN      ...         \n",
      "12960             NaN             NaN            NaN      ...         \n",
      "12961             NaN             NaN            NaN      ...         \n",
      "12962             NaN             NaN            NaN      ...         \n",
      "12963             NaN             NaN            NaN      ...         \n",
      "12964             NaN             NaN            NaN      ...         \n",
      "12965             NaN             NaN            NaN      ...         \n",
      "12966             NaN             NaN            NaN      ...         \n",
      "12967             NaN             NaN            NaN      ...         \n",
      "12968             NaN             NaN            NaN      ...         \n",
      "12969             NaN             NaN            NaN      ...         \n",
      "12970             NaN             NaN            NaN      ...         \n",
      "12971             NaN             NaN            NaN      ...         \n",
      "12972             NaN             NaN            NaN      ...         \n",
      "12973             NaN             NaN            NaN      ...         \n",
      "12974             NaN             NaN            NaN      ...         \n",
      "12975             NaN             NaN            NaN      ...         \n",
      "12976             NaN             NaN            NaN      ...         \n",
      "12977             NaN             NaN            NaN      ...         \n",
      "\n",
      "       rater2_trait3  rater2_trait4  rater2_trait5  rater2_trait6  \\\n",
      "0                NaN            NaN            NaN            NaN   \n",
      "1                NaN            NaN            NaN            NaN   \n",
      "2                NaN            NaN            NaN            NaN   \n",
      "3                NaN            NaN            NaN            NaN   \n",
      "4                NaN            NaN            NaN            NaN   \n",
      "5                NaN            NaN            NaN            NaN   \n",
      "6                NaN            NaN            NaN            NaN   \n",
      "7                NaN            NaN            NaN            NaN   \n",
      "8                NaN            NaN            NaN            NaN   \n",
      "9                NaN            NaN            NaN            NaN   \n",
      "10               NaN            NaN            NaN            NaN   \n",
      "11               NaN            NaN            NaN            NaN   \n",
      "12               NaN            NaN            NaN            NaN   \n",
      "13               NaN            NaN            NaN            NaN   \n",
      "14               NaN            NaN            NaN            NaN   \n",
      "15               NaN            NaN            NaN            NaN   \n",
      "16               NaN            NaN            NaN            NaN   \n",
      "17               NaN            NaN            NaN            NaN   \n",
      "18               NaN            NaN            NaN            NaN   \n",
      "19               NaN            NaN            NaN            NaN   \n",
      "20               NaN            NaN            NaN            NaN   \n",
      "21               NaN            NaN            NaN            NaN   \n",
      "22               NaN            NaN            NaN            NaN   \n",
      "23               NaN            NaN            NaN            NaN   \n",
      "24               NaN            NaN            NaN            NaN   \n",
      "25               NaN            NaN            NaN            NaN   \n",
      "26               NaN            NaN            NaN            NaN   \n",
      "27               NaN            NaN            NaN            NaN   \n",
      "28               NaN            NaN            NaN            NaN   \n",
      "29               NaN            NaN            NaN            NaN   \n",
      "...              ...            ...            ...            ...   \n",
      "12948            4.0            4.0            4.0            4.0   \n",
      "12949            3.0            3.0            3.0            3.0   \n",
      "12950            4.0            4.0            4.0            4.0   \n",
      "12951            4.0            4.0            4.0            3.0   \n",
      "12952            3.0            4.0            3.0            3.0   \n",
      "12953            5.0            5.0            4.0            4.0   \n",
      "12954            4.0            4.0            4.0            4.0   \n",
      "12955            4.0            4.0            4.0            3.0   \n",
      "12956            4.0            3.0            3.0            3.0   \n",
      "12957            4.0            4.0            4.0            4.0   \n",
      "12958            4.0            4.0            4.0            3.0   \n",
      "12959            5.0            4.0            5.0            4.0   \n",
      "12960            4.0            4.0            4.0            4.0   \n",
      "12961            4.0            4.0            4.0            4.0   \n",
      "12962            5.0            4.0            4.0            4.0   \n",
      "12963            4.0            4.0            4.0            4.0   \n",
      "12964            4.0            4.0            4.0            4.0   \n",
      "12965            4.0            4.0            3.0            3.0   \n",
      "12966            4.0            4.0            4.0            3.0   \n",
      "12967            4.0            4.0            4.0            4.0   \n",
      "12968            1.0            1.0            1.0            1.0   \n",
      "12969            4.0            4.0            3.0            3.0   \n",
      "12970            4.0            4.0            4.0            4.0   \n",
      "12971            4.0            3.0            3.0            3.0   \n",
      "12972            4.0            4.0            3.0            3.0   \n",
      "12973            4.0            4.0            4.0            3.0   \n",
      "12974            4.0            4.0            4.0            3.0   \n",
      "12975            5.0            5.0            5.0            5.0   \n",
      "12976            4.0            4.0            4.0            4.0   \n",
      "12977            4.0            4.0            4.0            4.0   \n",
      "\n",
      "       rater3_trait1  rater3_trait2  rater3_trait3  rater3_trait4  \\\n",
      "0                NaN            NaN            NaN            NaN   \n",
      "1                NaN            NaN            NaN            NaN   \n",
      "2                NaN            NaN            NaN            NaN   \n",
      "3                NaN            NaN            NaN            NaN   \n",
      "4                NaN            NaN            NaN            NaN   \n",
      "5                NaN            NaN            NaN            NaN   \n",
      "6                NaN            NaN            NaN            NaN   \n",
      "7                NaN            NaN            NaN            NaN   \n",
      "8                NaN            NaN            NaN            NaN   \n",
      "9                NaN            NaN            NaN            NaN   \n",
      "10               NaN            NaN            NaN            NaN   \n",
      "11               NaN            NaN            NaN            NaN   \n",
      "12               NaN            NaN            NaN            NaN   \n",
      "13               NaN            NaN            NaN            NaN   \n",
      "14               NaN            NaN            NaN            NaN   \n",
      "15               NaN            NaN            NaN            NaN   \n",
      "16               NaN            NaN            NaN            NaN   \n",
      "17               NaN            NaN            NaN            NaN   \n",
      "18               NaN            NaN            NaN            NaN   \n",
      "19               NaN            NaN            NaN            NaN   \n",
      "20               NaN            NaN            NaN            NaN   \n",
      "21               NaN            NaN            NaN            NaN   \n",
      "22               NaN            NaN            NaN            NaN   \n",
      "23               NaN            NaN            NaN            NaN   \n",
      "24               NaN            NaN            NaN            NaN   \n",
      "25               NaN            NaN            NaN            NaN   \n",
      "26               NaN            NaN            NaN            NaN   \n",
      "27               NaN            NaN            NaN            NaN   \n",
      "28               NaN            NaN            NaN            NaN   \n",
      "29               NaN            NaN            NaN            NaN   \n",
      "...              ...            ...            ...            ...   \n",
      "12948            NaN            NaN            NaN            NaN   \n",
      "12949            NaN            NaN            NaN            NaN   \n",
      "12950            NaN            NaN            NaN            NaN   \n",
      "12951            NaN            NaN            NaN            NaN   \n",
      "12952            NaN            NaN            NaN            NaN   \n",
      "12953            NaN            NaN            NaN            NaN   \n",
      "12954            NaN            NaN            NaN            NaN   \n",
      "12955            NaN            NaN            NaN            NaN   \n",
      "12956            NaN            NaN            NaN            NaN   \n",
      "12957            NaN            NaN            NaN            NaN   \n",
      "12958            NaN            NaN            NaN            NaN   \n",
      "12959            NaN            NaN            NaN            NaN   \n",
      "12960            NaN            NaN            NaN            NaN   \n",
      "12961            NaN            NaN            NaN            NaN   \n",
      "12962            NaN            NaN            NaN            NaN   \n",
      "12963            NaN            NaN            NaN            NaN   \n",
      "12964            4.0            4.0            4.0            4.0   \n",
      "12965            NaN            NaN            NaN            NaN   \n",
      "12966            NaN            NaN            NaN            NaN   \n",
      "12967            4.0            4.0            4.0            4.0   \n",
      "12968            NaN            NaN            NaN            NaN   \n",
      "12969            NaN            NaN            NaN            NaN   \n",
      "12970            NaN            NaN            NaN            NaN   \n",
      "12971            NaN            NaN            NaN            NaN   \n",
      "12972            3.0            3.0            4.0            4.0   \n",
      "12973            NaN            NaN            NaN            NaN   \n",
      "12974            NaN            NaN            NaN            NaN   \n",
      "12975            4.0            4.0            4.0            4.0   \n",
      "12976            NaN            NaN            NaN            NaN   \n",
      "12977            NaN            NaN            NaN            NaN   \n",
      "\n",
      "       rater3_trait5  rater3_trait6  \n",
      "0                NaN            NaN  \n",
      "1                NaN            NaN  \n",
      "2                NaN            NaN  \n",
      "3                NaN            NaN  \n",
      "4                NaN            NaN  \n",
      "5                NaN            NaN  \n",
      "6                NaN            NaN  \n",
      "7                NaN            NaN  \n",
      "8                NaN            NaN  \n",
      "9                NaN            NaN  \n",
      "10               NaN            NaN  \n",
      "11               NaN            NaN  \n",
      "12               NaN            NaN  \n",
      "13               NaN            NaN  \n",
      "14               NaN            NaN  \n",
      "15               NaN            NaN  \n",
      "16               NaN            NaN  \n",
      "17               NaN            NaN  \n",
      "18               NaN            NaN  \n",
      "19               NaN            NaN  \n",
      "20               NaN            NaN  \n",
      "21               NaN            NaN  \n",
      "22               NaN            NaN  \n",
      "23               NaN            NaN  \n",
      "24               NaN            NaN  \n",
      "25               NaN            NaN  \n",
      "26               NaN            NaN  \n",
      "27               NaN            NaN  \n",
      "28               NaN            NaN  \n",
      "29               NaN            NaN  \n",
      "...              ...            ...  \n",
      "12948            NaN            NaN  \n",
      "12949            NaN            NaN  \n",
      "12950            NaN            NaN  \n",
      "12951            NaN            NaN  \n",
      "12952            NaN            NaN  \n",
      "12953            NaN            NaN  \n",
      "12954            NaN            NaN  \n",
      "12955            NaN            NaN  \n",
      "12956            NaN            NaN  \n",
      "12957            NaN            NaN  \n",
      "12958            NaN            NaN  \n",
      "12959            NaN            NaN  \n",
      "12960            NaN            NaN  \n",
      "12961            NaN            NaN  \n",
      "12962            NaN            NaN  \n",
      "12963            NaN            NaN  \n",
      "12964            4.0            4.0  \n",
      "12965            NaN            NaN  \n",
      "12966            NaN            NaN  \n",
      "12967            4.0            4.0  \n",
      "12968            NaN            NaN  \n",
      "12969            NaN            NaN  \n",
      "12970            NaN            NaN  \n",
      "12971            NaN            NaN  \n",
      "12972            3.0            3.0  \n",
      "12973            NaN            NaN  \n",
      "12974            NaN            NaN  \n",
      "12975            4.0            4.0  \n",
      "12976            NaN            NaN  \n",
      "12977            NaN            NaN  \n",
      "\n",
      "[12978 rows x 28 columns]\n"
     ]
    }
   ],
   "source": [
    "#read in training data\n",
    "df = pd.read_excel(\"training_set_rel3.xls\")\n",
    "\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Putting essays, essay_set into dictionary and calculate each essay length\n",
    "train_data = {}\n",
    "for index, row in df.iterrows():\n",
    "    essay = row[\"essay\"].strip()#.split(\" \")\n",
    "    essay_set = row['essay_set']\n",
    "    domain1_score = row['domain1_score']/2\n",
    "    if essay_set not in train_data:\n",
    "        train_data[essay_set] = {\"essays\":[], \"score\":[]}\n",
    "\n",
    "    train_data[essay_set][\"essays\"].append(essay)\n",
    "    train_data[essay_set]['score'].append(domain1_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preperations\n",
    "Cleaning the essays and tokenizing the essays etc. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Extractions\n",
    "total word count, the number of sentences, word features (word length, word count per sentence, sd pf word count across sentences), spelling errors, number of lemmas, number of nouns, adjectives, adverbs and verbs.\n",
    "\n",
    "\n",
    "|Type of Features|| |\n",
    "|----------|-------------------|--------------|------|\n",
    "|Essay features|Total word count||\n",
    "|               |Number of Sentences||| \n",
    "|Sentence features|Average sentence length|||\n",
    "|     | SD of sentence length |  | |\n",
    "|Word features|Number of lemmas: |verb, noun, adj. adv. | |\n",
    "| | average word length (in characters)| ||\n",
    "|| BOW (TfidfVectorizer) |1 gram and 2 gram||\n",
    "||spelling error|||\n",
    "||Count of different words:|verb, noun, adj. adv.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def sentence_to_wordlist(raw_sentence):\n",
    "    clean_sentence = re.sub(\"[^a-zA-Z0-9]\", \" \", raw_sentence)\n",
    "    tokens = nltk.word_tokenize(clean_sentence)\n",
    "\n",
    "    return tokens\n",
    "\n",
    "\n",
    "def tokenize(essay):\n",
    "    stripped_essay = essay.strip()\n",
    "\n",
    "    tokenizer = nltk.data.load('tokenizers/punkt/english.pickle')\n",
    "    raw_sentences = tokenizer.tokenize(stripped_essay)\n",
    "\n",
    "    tokenized_sentences = []\n",
    "    for raw_sentence in raw_sentences:\n",
    "        if len(raw_sentence) > 0:\n",
    "            tokenized_sentences.append(sentence_to_wordlist(raw_sentence))\n",
    "\n",
    "    return tokenized_sentences\n",
    "\n",
    "\n",
    "def get_clean_essay(essay):\n",
    "    clean_essay = re.sub(r'\\W', ' ', essay)\n",
    "    return clean_essay\n",
    "\n",
    "#get list of sentences:\n",
    "def get_sent_list(essay):\n",
    "    sent_list = []\n",
    "    sentences = nltk.sent_tokenize(essay)\n",
    "    for sentence in sentences:\n",
    "        clean_sentence = re.sub(r'\\W', ' ', str(sentence).lower())\n",
    "        clean_sentence = re.sub(r'[0-9]', '', clean_sentence)\n",
    "\n",
    "        sent_list.append(clean_sentence)\n",
    "    return sent_list\n",
    "\n",
    "def get_big_dict():\n",
    "    big = open('big.txt').read()\n",
    "\n",
    "    words_ = re.findall('[a-z]+', big.lower())\n",
    "\n",
    "    big_dict = collections.defaultdict(lambda: 0)\n",
    "    #creating correct word dictionary\n",
    "    for word in words_:\n",
    "        big_dict[word] += 1\n",
    "    return(big_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#get word count discarding punctuations:\n",
    "def total_word_count(essay): #excluding punctuations\n",
    "    list_of_word_list = tokenize(essay)\n",
    "    flat_list_of_word = [w for l in list_of_word_list for w in l]\n",
    "    return len(flat_list_of_word)\n",
    "\n",
    "# get sentence\n",
    "def sent_num(essay):  #number of sentences in an essay\n",
    "    sentences_num = len(sent_tokenize(essay))\n",
    "    return sentences_num\n",
    "\n",
    "def word_feature(essay): #average word count and std of word count in sentence, avg word length throughout an essay\n",
    "    word_len =[]\n",
    "    words_in_sent = tokenize(essay)\n",
    "    for sent in words_in_sent:\n",
    "        word_len.extend([len(word) for word in sent])\n",
    "    avg_word_len = np.mean(word_len)\n",
    "    word_count_per_sentence = [len(s) for s in words_in_sent]\n",
    "    avg_wordcount = np.mean(word_count_per_sentence)\n",
    "    std_word_count =  np.std(word_count_per_sentence) #by sentence\n",
    "    return [avg_word_len, avg_wordcount, std_word_count]\n",
    "\n",
    "\n",
    "##number of lemmas:\n",
    "def count_lemmas(essay):\n",
    "    tokenized_sentences = tokenize(essay)\n",
    "\n",
    "    lemmas = []\n",
    "    wordnet_lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "    for sentence in tokenized_sentences:\n",
    "        tagged_tokens = nltk.pos_tag(sentence)\n",
    "\n",
    "        for token_tuple in tagged_tokens:\n",
    "\n",
    "            pos_tag = token_tuple[1]\n",
    "\n",
    "            if pos_tag.startswith('N'):\n",
    "                pos = wordnet.NOUN\n",
    "                lemmas.append(wordnet_lemmatizer.lemmatize(token_tuple[0], pos))\n",
    "            elif pos_tag.startswith('J'):\n",
    "                pos = wordnet.ADJ\n",
    "                lemmas.append(wordnet_lemmatizer.lemmatize(token_tuple[0], pos))\n",
    "            elif pos_tag.startswith('V'):\n",
    "                pos = wordnet.VERB\n",
    "                lemmas.append(wordnet_lemmatizer.lemmatize(token_tuple[0], pos))\n",
    "            elif pos_tag.startswith('R'):\n",
    "                pos = wordnet.ADV\n",
    "                lemmas.append(wordnet_lemmatizer.lemmatize(token_tuple[0], pos))\n",
    "            else:\n",
    "                pos = wordnet.NOUN\n",
    "                lemmas.append(wordnet_lemmatizer.lemmatize(token_tuple[0], pos))\n",
    "\n",
    "    lemma_count = len(set(lemmas))\n",
    "\n",
    "    return lemma_count\n",
    "\n",
    "\n",
    "##BOW\n",
    "def BOW(essay): ##essay is in the format of df[df['essay_set'] == 1]['essay']\n",
    "    #sentence = nltk.sent_tokenize(essay)  ##\n",
    "    vectorizer = TfidfVectorizer(ngram_range=(1, 2), max_df=1.0, min_df=1, max_features=10000,stop_words='english')\n",
    "    feature_matrix = vectorizer.fit_transform(essay)\n",
    "    feature_names = vectorizer.get_feature_names()\n",
    "    return feature_names, feature_matrix\n",
    "\n",
    "##Spelllng errors\n",
    "def count_spell_error(essay):\n",
    "    clean_essay = re.sub(r'\\W', ' ', str(essay).lower())\n",
    "    clean_essay = re.sub(r'[0-9]', '', clean_essay)\n",
    "\n",
    "    mispell_count = 0\n",
    "\n",
    "    words = clean_essay.split()\n",
    "\n",
    "    for word in words:\n",
    "        if not word in big_dict:\n",
    "            mispell_count += 1\n",
    "\n",
    "    return mispell_count\n",
    "\n",
    "###Number of Nouns, Verbs, adj, adv. in an essay\n",
    "def count_pos(essay):\n",
    "    tokenized_sentences = tokenize(essay)\n",
    "\n",
    "    noun_count = 0\n",
    "    adj_count = 0\n",
    "    verb_count = 0\n",
    "    adv_count = 0\n",
    "\n",
    "    for sentence in tokenized_sentences:\n",
    "        tagged_tokens = nltk.pos_tag(sentence)\n",
    "\n",
    "        for token_tuple in tagged_tokens:\n",
    "            pos_tag = token_tuple[1]\n",
    "\n",
    "            if pos_tag.startswith('N'):\n",
    "                noun_count += 1\n",
    "            elif pos_tag.startswith('J'):\n",
    "                adj_count += 1\n",
    "            elif pos_tag.startswith('V'):\n",
    "                verb_count += 1\n",
    "            elif pos_tag.startswith('R'):\n",
    "                adv_count += 1\n",
    "\n",
    "    return noun_count, adj_count, verb_count, adv_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting Features for Essay Set 1\n",
      "Extracting Features for Essay Set 2\n",
      "Extracting Features for Essay Set 3\n",
      "Extracting Features for Essay Set 4\n",
      "Extracting Features for Essay Set 5\n",
      "Extracting Features for Essay Set 6\n",
      "Extracting Features for Essay Set 7\n",
      "Extracting Features for Essay Set 8\n"
     ]
    }
   ],
   "source": [
    "def extract_features(essays, feature_functions):\n",
    "    return [[f(es) for f in feature_functions] for es in essays] ##list of list of features for each essay\n",
    "\n",
    "feature_functions = [total_word_count, sent_num, word_feature, count_lemmas, count_spell_error, count_pos]\n",
    "\n",
    "keys = [1,2,3,4,5,6,7,8]\n",
    "features = {key: [] for key in keys}\n",
    "big_dict = get_big_dict()\n",
    "for es_set in keys:\n",
    "    #if es_set not in BOW_dict:\n",
    "     #   BOW_dict={'es_set':[]}\n",
    "    #BOW_dict['es_set'].append([get_clean_essay(essay) for essay in train_data[es_set]['essays']])\n",
    "\n",
    "    print(\"Extracting Features for Essay Set %s\" % es_set)\n",
    "    #if es_set not in features:\n",
    "        #features={es_set :[]}\n",
    "    features[es_set].extend(extract_features(train_data[es_set][\"essays\"], feature_functions))\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "new_keys = [\"total word_count\",\"sentence_number\",\"word_features\",\"count_lemma\",\"spelling_error\",\"count_pos\"]\n",
    "Dict = {key: defaultdict(list) for key in new_keys}\n",
    "for key, value in features.items():\n",
    "    for v in value:\n",
    "        Dict['total word_count']['essay_set %s' % key].append(v[0])\n",
    "        Dict['sentence_number']['essay_set %s' % key].append(v[1])\n",
    "        Dict['word_features']['essay_set %s' % key].append(v[2])\n",
    "        Dict['count_lemma']['essay_set %s' % key].append(v[3])\n",
    "        Dict['spelling_error']['essay_set %s' % key].append(v[4])\n",
    "        Dict['count_pos']['essay_set %s' % key].append(v[5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import dill\n",
    "\n",
    "dill.dump(Dict, open('featurs_dict.pkd', 'wb'))\n",
    "dill.dump(features, open('features.pkd', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Dict = dill.load(open('featurs_dict.pkd', 'rb'))\n",
    "features = dill.load(open('features.pkd', 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from numpy import linspace\n",
    "from scipy.stats.kde import gaussian_kde\n",
    "\n",
    "from bokeh.io import output_file, show\n",
    "from bokeh.models import ColumnDataSource, FixedTicker, PrintfTickFormatter\n",
    "from bokeh.plotting import figure\n",
    "from bokeh.sampledata.perceptions import probly\n",
    "\n",
    "import colorcet as cc\n",
    "\n",
    "word_count_dict = Dict['total word_count']\n",
    "sentence_number_dict = Dict['sentence_number']\n",
    "word_features_dict = Dict['word_features']\n",
    "count_lemma_dict = Dict['count_lemma']\n",
    "spelling_error_dict = Dict['spelling_error']\n",
    "count_pos_dict = Dict['count_pos']\n",
    "\n",
    "# word_count=[]\n",
    "# Essay_sets=[]\n",
    "# for key in word_count_dict:\n",
    "#     n = len(word_count_dict[key])\n",
    "#     word_count.extend(word_count_dict[key])\n",
    "#     Essay_sets.extend(np.repeat(key, n))\n",
    "\n",
    "# df_word_count = pd.DataFrame({'Essay sets': Essay_sets, 'word count': word_count})\n",
    "\n",
    "def joy(category, data, scale=100):\n",
    "    return list(zip([category]*len(data), scale*data))\n",
    "\n",
    "cats = sorted(list(set(Essay_sets)))\n",
    "\n",
    "palette = [cc.rainbow[i*15] for i in range(17)]\n",
    "\n",
    "x = linspace(0,70, 500)\n",
    "\n",
    "Data = {'x': x}\n",
    "source = ColumnDataSource(Data)\n",
    "\n",
    "p1 = figure(y_range=cats, plot_width=900, x_range=(-5, 70), toolbar_location=None)\n",
    "\n",
    "for i, cat in enumerate(cats):\n",
    "    pdf = gaussian_kde(count_lemma_dict[cat])\n",
    "    y = joy(cat, pdf(x))\n",
    "    source.add(y, cat)\n",
    "    p1.patch('x', cat, color=palette[i], alpha=0.7, line_color=\"black\", source=source)\n",
    "\n",
    "p1.title.text = \"The Number of Sentences per Essay Category\"\n",
    "\n",
    "p1.title.align = 'center'\n",
    "p1.title.text_font_size = '20pt'\n",
    "p1.title.text_font = 'serif'\n",
    "\n",
    "    # Axis titles\n",
    "p1.xaxis.axis_label_text_font_size = '14pt'\n",
    "p1.xaxis.axis_label_text_font_style = 'bold'\n",
    "p1.yaxis.axis_label_text_font_size = '14pt'\n",
    "p1.yaxis.axis_label_text_font_style = 'bold'\n",
    "\n",
    "    # Tick labels\n",
    "p1.xaxis.major_label_text_font_size = '12pt'\n",
    "p1.yaxis.major_label_text_font_size = '12pt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/pengluyao/anaconda3/lib/python3.6/importlib/_bootstrap.py:219: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility. Expected 96, got 88\n  return f(*args, **kwds)\n"
     ]
    },
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'bokeh.models'; 'bokeh' is not a package",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-f208288dc4ed>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mbokeh\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mio\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0moutput_notebook\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0moutput_notebook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/PycharmProjects/Capstone/bokeh.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mscipy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstats\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkde\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mgaussian_kde\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdill\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mbokeh\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodels\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mColumnDataSource\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mFixedTicker\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mPrintfTickFormatter\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mSelect\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mPlot\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mbokeh\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplotting\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mfigure\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mbokeh\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mglyphs\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mPatches\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'bokeh.models'; 'bokeh' is not a package"
     ],
     "output_type": "error"
    }
   ],
   "source": [
    "from bokeh.io import output_notebook\n",
    "output_notebook()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'show' is not defined",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-4b1af81f547f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mp1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'show' is not defined"
     ],
     "output_type": "error"
    }
   ],
   "source": [
    "show(p1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'linspace' is not defined",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-f72557a326b4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcategory\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscale\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlinspace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m900\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m500\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0mData\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m'x'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'linspace' is not defined"
     ],
     "output_type": "error"
    }
   ],
   "source": [
    "\n",
    "def joy(category, data, scale=200):\n",
    "    return list(zip([category]*len(data), scale*data))\n",
    "\n",
    "x = linspace(0,900, 500)\n",
    "\n",
    "Data = {'x': x}\n",
    "source = ColumnDataSource(Data)\n",
    "\n",
    "p2 = figure(y_range=cats, plot_width=900, x_range=(-5, 900), toolbar_location=None)\n",
    "\n",
    "for i, cat in enumerate(cats):\n",
    "    pdf = gaussian_kde(word_count_dict[cat])\n",
    "    y = joy(cat, pdf(x))\n",
    "    source.add(y, cat)\n",
    "    p2.patch('x', cat, color=palette[i], alpha=0.7, line_color=\"black\", source=source)\n",
    "\n",
    "p2.title.text = \"The Total Word Count\"\n",
    "\n",
    "p2.title.align = 'center'\n",
    "p2.title.text_font_size = '20pt'\n",
    "p2.title.text_font = 'serif'\n",
    "\n",
    "    # Axis titles\n",
    "p2.xaxis.axis_label_text_font_size = '14pt'\n",
    "p2.xaxis.axis_label_text_font_style = 'bold'\n",
    "p2.yaxis.axis_label_text_font_size = '14pt'\n",
    "p2.yaxis.axis_label_text_font_style = 'bold'\n",
    "\n",
    "    # Tick labels\n",
    "p2.xaxis.major_label_text_font_size = '12pt'\n",
    "p2.yaxis.major_label_text_font_size = '12pt'\n",
    "\n",
    "show(p2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
